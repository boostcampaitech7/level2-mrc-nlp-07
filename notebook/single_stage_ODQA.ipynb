{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notebook/requirements.txt: EDA를 위한 종속성 관리\n",
    "- autoEDA: llama2로 특수 문자 여부를 판단한 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/train_dataset/train/dataset.arrow'\n",
    "with pa.memory_map(file_path, 'r') as source:\n",
    "    table = pa.ipc.open_stream(source).read_all()\n",
    "df: pd.DataFrame = table.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* 로컬 Ollama 서버로 llama2 활용하는 방법 (macOS 기준)\n",
    "1. Ollama 설치\n",
    "    ```\n",
    "    brew install ollama\n",
    "    ```\n",
    "2. Ollama 서버 실행\n",
    "    ```\n",
    "    ollama run llama2\n",
    "    ```\n",
    "3. Ollama 서버 중단\n",
    "    ```\n",
    "    pkill ollama\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt_template = \"\"\"\n",
    "다음 문서를 분석하고 주어진 질문에 답변할 수 있도록 필요한 정보를 요약해 주세요. 문서의 논리 구조와 질문의 연관성을 중점적으로 파악하고, 관련 없는 세부 사항은 제외하세요.\n",
    "\n",
    "### 문서:\n",
    "{context}\n",
    "\n",
    "### 질문:\n",
    "{question}\n",
    "\n",
    "문서를 **한국어로 요약**하고, 질문에 대한 답변에 필요한 정보만 포함해 주세요.\n",
    "\"\"\"\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=summary_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_prompt_template = \"\"\"\n",
    "요약된 문서를 바탕으로 주어진 질문에 **오직 한국어로** 짧고 간결하게 대답해 주세요. 답변은 **한 단어나 짧은 구절**로 이루어져야 합니다.\n",
    "\n",
    "### 요약된 문서:\n",
    "{summary}\n",
    "\n",
    "### 질문:\n",
    "{question}\n",
    "\n",
    "**한국어로** 가장 중요한 한 단어나 구절로 대답해 주세요. **영어로 대답하지 마세요.**\n",
    "\"\"\"\n",
    "answer_prompt = PromptTemplate(\n",
    "    input_variables=[\"summary\", \"question\"],\n",
    "    template=answer_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompt_template = \"\"\"\n",
    "생성된 답변을 검토하여 다음 기준을 충족하는지 확인하세요:\n",
    "1. 답변은 **한 단어나 짧은 구절**로, **한국어로만** 이루어져야 합니다.\n",
    "2. 답변은 가능한 한 짧고 명확하게 작성되어야 합니다.\n",
    "3. 질문에 직접적으로 관련된 내용만 포함되어야 하며, 불필요한 정보는 제외해야 합니다.\n",
    "\n",
    "### 생성된 답변:\n",
    "{answer}\n",
    "\n",
    "답변이 너무 길거나 줄일 수 있는 경우, **한국어로** 더 짧고 간결하게 수정하세요. 최종 답변은 짧고 구체적이며 질문에 정확히 대답할 수 있는 **한국어** 단어로 작성되어야 합니다.\n",
    "\"\"\"\n",
    "final_prompt = PromptTemplate(\n",
    "    input_variables=[\"answer\"],\n",
    "    template=final_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 프롬프트를 체인으로 연결\n",
    "summary_chain = (\n",
    "    {\n",
    "        \"context\": RunnablePassthrough(),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | summary_prompt\n",
    "    | llm\n",
    "    | JsonOutputParser()\n",
    ")\n",
    "\n",
    "answer_chain = (\n",
    "    {\n",
    "        \"summary\": RunnablePassthrough(),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | answer_prompt\n",
    "    | llm\n",
    "    | JsonOutputParser()\n",
    ")\n",
    "\n",
    "final_chain = (\n",
    "    {\n",
    "        \"answer\": RunnablePassthrough(),\n",
    "    }\n",
    "    | final_prompt\n",
    "    | llm\n",
    "    | JsonOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_korean_answer(s):\n",
    "    def remove_punctuation(text):\n",
    "        exclude = set(string.punctuation + \"·、，．？！＂＇〃《》「」『』〔〕“”‘’〈〉【】()[]{}\")\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_punctuation(lower(s)))\n",
    "\n",
    "def exact_match(prediction, ground_truth):\n",
    "    return normalize_korean_answer(prediction) == normalize_korean_answer(ground_truth)\n",
    "\n",
    "# F1 Score calculation\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_korean_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_korean_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "\n",
    "    precision = num_same / len(prediction_tokens)\n",
    "    recall = num_same / len(ground_truth_tokens)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_matches = []\n",
    "f1_scores = []\n",
    "\n",
    "for idx in range(len(df)):\n",
    "    context = df.iloc[idx]['context']\n",
    "    question = df.iloc[idx]['question']\n",
    "    answer = df.iloc[idx]['answers']['text'][0]  # 정답 추출\n",
    "\n",
    "    # Step 1: 문서 요약\n",
    "    summary_result = summary_chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "\n",
    "    # Step 2: 요약을 바탕으로 답변 생성\n",
    "    answer_result = answer_chain.invoke({\n",
    "        \"summary\": summary_result['answer'],\n",
    "        \"question\": question\n",
    "    })\n",
    "\n",
    "    # Step 3: 최종 답변 검토 및 출력\n",
    "    final_answer = final_chain.invoke({\n",
    "        \"answer\": answer_result['answer'],\n",
    "    })\n",
    "\n",
    "    predicted_answer = final_answer['answer']\n",
    "    \n",
    "    # 정확도 평가 (Exact Match와 F1 Score)\n",
    "    em = exact_match(predicted_answer, answer)\n",
    "    f1 = f1_score(predicted_answer, answer)\n",
    "\n",
    "    # 메트릭 저장\n",
    "    exact_matches.append(em)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # 출력\n",
    "    print(f\"{idx}.\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predicted_answer}\")\n",
    "    print(f\"True Answer: {answer}\")\n",
    "    print(f\"Exact Match: {em}, F1 score: {f1}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# 10. 전체 평균 메트릭 계산\n",
    "avg_em = np.mean(exact_matches)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "print(f\"Average Exact Match: {avg_em:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
