{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "        num_rows: 3952\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "})\n",
      "**************************************** query dataset ****************************************\n",
      "Dataset({\n",
      "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "    num_rows: 4192\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, load_from_disk\n",
    "from src.retriever.retrieval.sparse_retrieval import SparseRetrieval\n",
    "from src.retriever.embedding.flag_embedding import DenseRetrieval\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from src.retriever.score.ranking import check_original_in_context, calculate_reverse_rank_score, calculate_linear_score\n",
    "org_dataset = load_from_disk('./data/train_dataset')\n",
    "print(org_dataset)\n",
    "full_ds = concatenate_datasets(\n",
    "        [\n",
    "            org_dataset[\"train\"].flatten_indices(),\n",
    "            org_dataset[\"validation\"].flatten_indices(),\n",
    "        ]\n",
    "    )  # train dev 를 합친 4192 개 질문에 대해 모두 테스트\n",
    "print(\"*\" * 40, \"query dataset\", \"*\" * 40)\n",
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing KoELECTRA...\n",
      "Lengths of unique contexts : 56737\n",
      "Building bm25 embedding...\n",
      "Start Initializing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...: 100%|██████████| 56737/56737 [01:13<00:00, 776.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating n-grams and building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-grams: 100%|██████████| 56737/56737 [00:23<00:00, 2401.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mode : bm25\n",
      "End Initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25: 100%|██████████| 57/57 [10:46<00:00, 11.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish BM25 Embedding\n",
      "bm25 embedding shape: (56737, 1000000)\n",
      "(4192, 1000000) (56737, 1000000)\n",
      "result shape : (4192, 56737)\n",
      "[query exhaustive search] done in 38.100 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval: 100%|██████████| 4192/4192 [00:00<00:00, 12807.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct retrieval 0.8752385496183206\n",
      "reverse rank retrieval 0.5425256927956311\n",
      "linear retrieval 0.8051138530275673\n",
      "Processing Mecab...\n",
      "Lengths of unique contexts : 56737\n",
      "Building bm25 embedding...\n",
      "Start Initializing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...:   0%|          | 0/56737 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Install MeCab in order to use it: http://konlpy.org/en/latest/install/",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\승범 pc\\Desktop\\study\\deeplearning\\naver_boot\\level2-mrc-nlp-07\\level2\\Lib\\site-packages\\konlpy\\tag\\_mecab.py:77\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagger \u001b[38;5;241m=\u001b[39m \u001b[43mTagger\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-d \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dicpath)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagset \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m/data/tagset/mecab.json\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m utils\u001b[38;5;241m.\u001b[39minstallpath)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tagger' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 56\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Define tokenizers to compare\u001b[39;00m\n\u001b[0;32m     47\u001b[0m tokenizers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m#(\"KLUE RoBERTa Large\", AutoTokenizer.from_pretrained(\"klue/roberta-large\", use_fast=False)),\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m#(\"KLUE RoBERTa Base\", AutoTokenizer.from_pretrained(\"klue/roberta-base\", use_fast=False)),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMecab\u001b[39m\u001b[38;5;124m\"\u001b[39m, mecab_tokenizer),\n\u001b[0;32m     55\u001b[0m ]\n\u001b[1;32m---> 56\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_tokenizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mcompare_tokenizers\u001b[1;34m(tokenizers, full_ds)\u001b[0m\n\u001b[0;32m     16\u001b[0m retriever \u001b[38;5;241m=\u001b[39m SparseRetrieval(\n\u001b[0;32m     17\u001b[0m     tokenize_fn\u001b[38;5;241m=\u001b[39mtokenizer,\u001b[38;5;66;03m#.tokenize,\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#tokenized_docs = tokenized_docs,\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Update tokenizer in retriever\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Get sparse embedding and retrieve\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sparse_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m df \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mretrieve(full_ds, topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Get scores\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\승범 pc\\Desktop\\study\\deeplearning\\naver_boot\\level2-mrc-nlp-07\\src\\retriever\\retrieval\\sparse_retrieval.py:99\u001b[0m, in \u001b[0;36mSparseRetrieval.get_sparse_embedding\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# if os.path.isfile(self.emd_path) and os.path.isfile(self.sparse_path):\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m#     print(f\"Loading {self.mode} embedding...\")\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#     self.p_embedding = load_npz(self.emd_path)\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m#     self.sparse_embed = SparseEmbedding.load(self.sparse_path)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m#     print(\"Loading completed.\")\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m embedding...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m embedding shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_embedding\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\승범 pc\\Desktop\\study\\deeplearning\\naver_boot\\level2-mrc-nlp-07\\src\\retriever\\retrieval\\sparse_retrieval.py:104\u001b[0m, in \u001b[0;36mSparseRetrieval._calculate_embeddings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_embed \u001b[38;5;241m=\u001b[39m \u001b[43mSparseEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mngram_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mngram_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenized_docs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenized_docs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_embed\u001b[38;5;241m.\u001b[39mget_embedding()\n",
      "File \u001b[1;32mc:\\Users\\승범 pc\\Desktop\\study\\deeplearning\\naver_boot\\level2-mrc-nlp-07\\src\\retriever\\embedding\\sparse_embedding.py:42\u001b[0m, in \u001b[0;36mSparseEmbedding.__init__\u001b[1;34m(self, docs, tokenizer, ngram_range, max_features, mode, tokenized_docs, k1, b)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart Initializing...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenized_docs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(docs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGenerating n-grams and building vocabulary...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_ngrams_and_update_vocab()\n",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m, in \u001b[0;36mmecab_tokenizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmecab_tokenizer\u001b[39m(text):\n\u001b[1;32m----> 8\u001b[0m     mecab \u001b[38;5;241m=\u001b[39m \u001b[43mMecab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mecab\u001b[38;5;241m.\u001b[39mmorphs(text)\n",
      "File \u001b[1;32mc:\\Users\\승범 pc\\Desktop\\study\\deeplearning\\naver_boot\\level2-mrc-nlp-07\\level2\\Lib\\site-packages\\konlpy\\tag\\_mecab.py:82\u001b[0m, in \u001b[0;36mMecab.__init__\u001b[1;34m(self, dicpath)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe MeCab dictionary does not exist at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Is the dictionary correctly installed?\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou can also try entering the dictionary path when initializing the Mecab class: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMecab(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m/some/dic/path\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dicpath)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstall MeCab in order to use it: http://konlpy.org/en/latest/install/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Install MeCab in order to use it: http://konlpy.org/en/latest/install/"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from konlpy.tag import Okt, Kkma, Komoran, Hannanum, Mecab\n",
    "\n",
    "def okt_tokenizer(text):\n",
    "    okt = Okt()\n",
    "    return okt.morphs(text)\n",
    "\n",
    "def kkma_tokenizer(text):\n",
    "    kkma = Kkma()\n",
    "    return kkma.morphs(text)\n",
    "\n",
    "def komoran_tokenizer(text):\n",
    "    komoran = Komoran()\n",
    "    return komoran.morphs(text)\n",
    "\n",
    "def hannanum_tokenizer(text):\n",
    "    hannanum = Hannanum()\n",
    "    return hannanum.morphs(text)\n",
    "def mecab_tokenizer(text):\n",
    "    mecab = Mecab()\n",
    "    return mecab.morphs(text)\n",
    "\n",
    "def compare_tokenizers(tokenizers: List[Tuple[str, AutoTokenizer]],  full_ds):\n",
    "    results = {}\n",
    "    \n",
    "    for name, tokenizer in tokenizers:\n",
    "        print(f\"Processing {name}...\")\n",
    "        retriever = SparseRetrieval(\n",
    "            tokenize_fn=tokenizer,#.tokenize,\n",
    "            data_path=\"./data/\",\n",
    "            context_path=\"wikipedia_documents.json\",\n",
    "            mode = \"bm25\",\n",
    "            max_feature=1000000,\n",
    "            ngram_range=(1,2),\n",
    "            #tokenized_docs = tokenized_docs,\n",
    "        )\n",
    "        # Update tokenizer in retriever\n",
    "        # Get sparse embedding and retrieve\n",
    "        retriever.get_sparse_embedding()\n",
    "        df = retriever.retrieve(full_ds, topk=10)\n",
    "        \n",
    "        # Get scores\n",
    "        scores = retriever.get_score(df)\n",
    "        results[name] = scores\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_results(results):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, scores in results.items():\n",
    "        plt.plot(scores, label=name)\n",
    "    plt.xlabel('Query Index')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Tokenizer Comparison for Retrieval')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Define tokenizers to compare\n",
    "tokenizers = [\n",
    "    #(\"KLUE RoBERTa Large\", AutoTokenizer.from_pretrained(\"klue/roberta-large\", use_fast=False)),\n",
    "    #(\"KLUE RoBERTa Base\", AutoTokenizer.from_pretrained(\"klue/roberta-base\", use_fast=False)),\n",
    "    #(\"KLUE RoBERTa Small\", AutoTokenizer.from_pretrained(\"klue/roberta-small\", use_fast=False)),\n",
    "    #(\"KoBERT\", AutoTokenizer.from_pretrained(\"skt/kobert-base-v1\", use_fast=False).tokenize),\n",
    "    (\"KoELECTRA\", AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", use_fast=False).tokenize),\n",
    "    #(\"KoGPT2\", AutoTokenizer.from_pretrained(\"skt/kogpt2-base-v2\", use_fast=False).tokenize),\n",
    "    (\"Mecab\", mecab_tokenizer),\n",
    "    (\"Okt\", okt_tokenizer),\n",
    "    (\"Kkma\", kkma_tokenizer),\n",
    "    (\"Komoran\", komoran_tokenizer),\n",
    "    (\"Hannanum\", hannanum_tokenizer),\n",
    "]\n",
    "results = compare_tokenizers(tokenizers, full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 선언한거 가져오기 Retriever\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", use_fast=False,)\n",
    "retriever = SparseRetrieval(\n",
    "    tokenize_fn=tokenizer.tokenize,\n",
    "    data_path=\"./data/\",\n",
    "    context_path=\"wikipedia_documents.json\",\n",
    "    mode = \"bm25\",\n",
    "    max_feature=1000000,\n",
    "    ngram_range=(1,3),\n",
    "    #tokenized_docs = tokenized_docs,\n",
    ")\n",
    "retriever.get_sparse_embedding()\n",
    "df = retriever.retrieve(full_ds, topk=10)\n",
    "retriever.get_score(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "docs = list(dict.fromkeys([v[\"text\"] for v in wiki.values()]))\n",
    "print(f\"Lengths of unique contexts : {len(docs)}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", use_fast=False,)\n",
    "tokenized_docs = [tokenizer(doc) for doc in tqdm(docs, desc=\"Tokenizing...\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = SparseRetrieval(\n",
    "            tokenize_fn=tokenizer.tokenize,\n",
    "            data_path=\"./data/\",\n",
    "            context_path=\"wikipedia_documents.json\",\n",
    "            mode = \"bm25\",\n",
    "            max_feature=200000,\n",
    "            ngram_range=(1,2),\n",
    "            tokenized_docs = tokenized_docs,\n",
    "            b=0.25,\n",
    "            k1=1.1,\n",
    "        )\n",
    "retriever.get_sparse_embedding()\n",
    "df = retriever.retrieve(full_ds, topk=10)\n",
    "retriever.get_score(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "level2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
