{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Dataset 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "        num_rows: 3952\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "})\n",
      "**************************************** query dataset ****************************************\n",
      "Dataset({\n",
      "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "    num_rows: 4192\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, load_from_disk\n",
    "from src.retriever.retrieval.sparse_retrieval import SparseRetrieval\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "org_dataset = load_from_disk('data/train_dataset')\n",
    "print(org_dataset)\n",
    "full_ds = concatenate_datasets(\n",
    "        [\n",
    "            org_dataset[\"train\"].flatten_indices(),\n",
    "            org_dataset[\"validation\"].flatten_indices(),\n",
    "        ]\n",
    "    )  # train dev 를 합친 4192 개 질문에 대해 모두 테스트\n",
    "print(\"*\" * 40, \"query dataset\", \"*\" * 40)\n",
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever 선언 및 Retriever 진행\n",
    "* 아마 초기엔 학습 진행하는데 시간 좀 걸릴예정\n",
    "    * 약 15분정도 소요(승범 컴 기준)\n",
    "    * 노션에 mrc/검증결과/Sparse 처리에 가장 좋은 bm25올려났으므로 그거 data 폴더 안에 넣으면 실행가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n",
      "Building bm25 embedding...\n",
      "Start Initializing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...: 100%|██████████| 56737/56737 [02:22<00:00, 397.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating n-grams and building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-grams: 100%|██████████| 56737/56737 [00:34<00:00, 1623.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mode : bm25\n",
      "End Initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25: 100%|██████████| 57/57 [23:35<00:00, 24.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish BM25 Embedding\n",
      "bm25 embedding shape: (56737, 1000000)\n",
      "(4192, 1000000) (56737, 1000000)\n",
      "<class 'scipy.sparse._csr.csr_matrix'> <class 'scipy.sparse._csr.csr_matrix'>\n",
      "result shape : (4192, 56737)\n",
      "[query exhaustive search] done in 59.492 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval: 100%|██████████| 4192/4192 [00:00<00:00, 7737.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>id</th>\n",
       "      <th>retrieval_context</th>\n",
       "      <th>original_context</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?</td>\n",
       "      <td>mrc-1-000067</td>\n",
       "      <td>이하의 국가들은 의회에서 행정부 수반을 선출한다는 점에서 내각제와 닮았다. 그리고 ...</td>\n",
       "      <td>미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...</td>\n",
       "      <td>{'answer_start': [235], 'text': ['하원']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>현대적 인사조직관리의 시발점이 된 책은?</td>\n",
       "      <td>mrc-0-004397</td>\n",
       "      <td>'근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...</td>\n",
       "      <td>'근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...</td>\n",
       "      <td>{'answer_start': [212], 'text': ['《경영의 실제》']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>강희제가 1717년에 쓴 글은 누구를 위해 쓰여졌는가?</td>\n",
       "      <td>mrc-1-000362</td>\n",
       "      <td>1943년 초, 마리아 발토르타가 9년 간 건강 상태가 안 좋아졌을 때, 그녀의 고...</td>\n",
       "      <td>강희제는 강화된 황권으로 거의 황제 중심의 독단적으로 나라를 이끌어 갔기에 자칫 전...</td>\n",
       "      <td>{'answer_start': [510], 'text': ['백성']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11~12세기에 제작된 본존불은 보통 어떤 나라의 특징이 전파되었나요?</td>\n",
       "      <td>mrc-0-001510</td>\n",
       "      <td>이 불상군은 1978년 발견되어 학계에 알려졌으며 봉황리 햇골산 중턱 두 곳에 약간...</td>\n",
       "      <td>불상을 모시기 위해 나무나 돌, 쇠 등을 깎아 일반적인 건축물보다 작은 규모로 만든...</td>\n",
       "      <td>{'answer_start': [625], 'text': ['중국']}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>명문이 적힌 유물을 구성하는 그릇의 총 개수는?</td>\n",
       "      <td>mrc-0-000823</td>\n",
       "      <td>양산 통도사의 금동천문도의 전면에는 천구(天球)의 북극을 중심으로 둥글게 북극으로 ...</td>\n",
       "      <td>동아대학교박물관에서 소장하고 있는 계사명 사리구는 총 4개의 용기로 구성된 조선후기...</td>\n",
       "      <td>{'answer_start': [30], 'text': ['4개']}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  question            id  \\\n",
       "0         대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?  mrc-1-000067   \n",
       "1                   현대적 인사조직관리의 시발점이 된 책은?  mrc-0-004397   \n",
       "2           강희제가 1717년에 쓴 글은 누구를 위해 쓰여졌는가?  mrc-1-000362   \n",
       "3  11~12세기에 제작된 본존불은 보통 어떤 나라의 특징이 전파되었나요?  mrc-0-001510   \n",
       "4               명문이 적힌 유물을 구성하는 그릇의 총 개수는?  mrc-0-000823   \n",
       "\n",
       "                                   retrieval_context  \\\n",
       "0  이하의 국가들은 의회에서 행정부 수반을 선출한다는 점에서 내각제와 닮았다. 그리고 ...   \n",
       "1  '근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...   \n",
       "2  1943년 초, 마리아 발토르타가 9년 간 건강 상태가 안 좋아졌을 때, 그녀의 고...   \n",
       "3  이 불상군은 1978년 발견되어 학계에 알려졌으며 봉황리 햇골산 중턱 두 곳에 약간...   \n",
       "4  양산 통도사의 금동천문도의 전면에는 천구(天球)의 북극을 중심으로 둥글게 북극으로 ...   \n",
       "\n",
       "                                    original_context  \\\n",
       "0  미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국...   \n",
       "1  '근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 19...   \n",
       "2  강희제는 강화된 황권으로 거의 황제 중심의 독단적으로 나라를 이끌어 갔기에 자칫 전...   \n",
       "3  불상을 모시기 위해 나무나 돌, 쇠 등을 깎아 일반적인 건축물보다 작은 규모로 만든...   \n",
       "4  동아대학교박물관에서 소장하고 있는 계사명 사리구는 총 4개의 용기로 구성된 조선후기...   \n",
       "\n",
       "                                         answers  \n",
       "0        {'answer_start': [235], 'text': ['하원']}  \n",
       "1  {'answer_start': [212], 'text': ['《경영의 실제》']}  \n",
       "2        {'answer_start': [510], 'text': ['백성']}  \n",
       "3        {'answer_start': [625], 'text': ['중국']}  \n",
       "4         {'answer_start': [30], 'text': ['4개']}  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 위에서 선언한거 가져오기 Retriever\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", use_fast=False,)\n",
    "retriever = SparseRetrieval(\n",
    "    tokenize_fn=tokenizer.tokenize,\n",
    "    data_path=\"./data/\",\n",
    "    context_path=\"wikipedia_documents.json\",\n",
    "    mode = \"bm25\",\n",
    "    max_feature=1000000,\n",
    "    ngram_range=(1,2),\n",
    "    #tokenized_docs = tokenized_docs,\n",
    ")\n",
    "retriever.get_sparse_embedding()\n",
    "\n",
    "# Top k 조절.\n",
    "df = retriever.retrieve(full_ds, topk=10)\n",
    "\n",
    "# 여기서 df가 reader가 사용할 데이터 프레임입니다.\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* question : 질문\n",
    "* id : mrc 번호\n",
    "* retrieval_context : retrieval 결과로 나온 topk를 하나의 text로 만든것\n",
    "* original_context : 실제 retrieval 정답\n",
    "* answers: dict 형태\n",
    "    * answers_start : original_context에서 어디에서 시작하는지\n",
    "    * text: 정답의 text값이 뭔지 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13059"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['retrieval_context'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever 결과물 DatasetDict로 저장\n",
    "reader는 데이터셋을 DatasetDict형태로 받으므로, df에서 변환이 필요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'id', 'context', 'answers'],\n",
      "        num_rows: 4192\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'id', 'context', 'answers'],\n",
      "        num_rows: 4192\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# df를 Dataset으로\n",
    "retriever_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# DatasetDict로 변환 (train과 validation을 동일 데이터로 초기화)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': retriever_dataset,\n",
    "    'validation': retriever_dataset\n",
    "})\n",
    "\n",
    "# train 스플릿: 'original_context'를 'context'로 사용\n",
    "dataset_dict['train'] = dataset_dict['train'].remove_columns(['retrieval_context']).rename_column('original_context', 'context')\n",
    "\n",
    "# validation 스플릿: 'retrieval_context'를 'context'로 사용\n",
    "dataset_dict['validation'] = dataset_dict['validation'].remove_columns(['original_context']).rename_column('retrieval_context', 'context')\n",
    "\n",
    "print(dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4192/4192 [00:00<00:00, 238899.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4192/4192 [00:00<00:00, 38603.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 별도 파일로 저장\n",
    "dataset_dict.save_to_disk('outputs/bm25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4192/4192 [00:04<00:00, 977.83 examples/s] \n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
      "    num_rows: 8452\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1587' max='1587' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1587/1587 09:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.371300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.171400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 23:21:02,029 - klue/bert-base - INFO - ***** Train results *****\n",
      "2024-10-23 23:21:02,029 - klue/bert-base - INFO - epoch = 3.0\n",
      "2024-10-23 23:21:02,030 - klue/bert-base - INFO - total_flos = 4969033471087536.0\n",
      "2024-10-23 23:21:02,030 - klue/bert-base - INFO - train_loss = 0.6351578092905584\n",
      "2024-10-23 23:21:02,030 - klue/bert-base - INFO - train_runtime = 552.9956\n",
      "2024-10-23 23:21:02,031 - klue/bert-base - INFO - train_samples = 8452\n",
      "2024-10-23 23:21:02,031 - klue/bert-base - INFO - train_samples_per_second = 45.852\n",
      "2024-10-23 23:21:02,032 - klue/bert-base - INFO - train_steps_per_second = 2.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               =  4627773GF\n",
      "  train_loss               =     0.6352\n",
      "  train_runtime            = 0:09:12.99\n",
      "  train_samples            =       8452\n",
      "  train_samples_per_second =     45.852\n",
      "  train_steps_per_second   =       2.87\n"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser, TrainingArguments\n",
    "from datasets import DatasetDict\n",
    "from src.reader.model.reader import Reader\n",
    "from src.utils.arguments import DataTrainingArguments, ModelArguments\n",
    "\n",
    "# Argument 설정\n",
    "model_args = ModelArguments()\n",
    "data_args = DataTrainingArguments()\n",
    "training_args = TrainingArguments(output_dir='outputs/models/train_dataset')\n",
    "\n",
    "# 학습 관련 설정\n",
    "training_args.do_train = True\n",
    "training_args.do_eval = False\n",
    "training_args.do_predict = False\n",
    "\n",
    "# 학습 하이퍼파라미터 설정\n",
    "model_args.model_name_or_path = 'klue/bert-base'  # 학습에 사용할 모델\n",
    "training_args.learning_rate = 5e-5  # 학습률\n",
    "training_args.num_train_epochs = 3  # epoch 수\n",
    "training_args.per_device_train_batch_size = 16  # 학습 배치 사이즈\n",
    "\n",
    "# 데이터셋 로드 (BM25 기반으로 생성된 데이터셋)\n",
    "dataset = DatasetDict.load_from_disk('outputs/bm25')\n",
    "\n",
    "# Reader 모델 학습 실행\n",
    "reader_model = Reader(model_args, data_args, training_args, dataset)\n",
    "reader_model.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4192/4192 [01:21<00:00, 51.65 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
      "    num_rows: 109120\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:483: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13640' max='13640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13640/13640 13:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postprocess_qa Dataset({\n",
      "    features: ['question', 'id', 'context', 'answers'],\n",
      "    num_rows: 4192\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4192/4192 [05:35<00:00, 12.49it/s]\n",
      "2024-10-23 23:45:18,562 - postprocess log - INFO - Saving predictions to outputs/train_dataset/predictions.json.\n",
      "2024-10-23 23:45:18,570 - postprocess log - INFO - Saving nbest_preds to outputs/train_dataset/nbest_predictions.json.\n",
      "2024-10-23 23:45:20,409 - outputs/models/train_dataset - INFO - Evaluation results: {'exact_match': 78.74522900763358, 'f1': 80.33898883135521}\n"
     ]
    }
   ],
   "source": [
    "# Argument 설정\n",
    "model_args = ModelArguments()\n",
    "data_args = DataTrainingArguments()\n",
    "training_args = TrainingArguments(output_dir='outputs/train_dataset')\n",
    "\n",
    "# 평가 관련 설정\n",
    "training_args.do_train = False\n",
    "training_args.do_eval = True\n",
    "training_args.do_predict = False\n",
    "\n",
    "# 학습된 모델 경로 설정\n",
    "model_args.model_name_or_path = 'outputs/models/train_dataset'\n",
    "\n",
    "# 평가 데이터셋 로드 (BM25 기반으로 생성된 데이터셋)\n",
    "dataset = DatasetDict.load_from_disk('outputs/bm25')\n",
    "\n",
    "# Reader 모델 평가 실행\n",
    "reader_model = Reader(model_args, data_args, training_args, dataset)\n",
    "reader_model.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 600/600 [00:00<00:00, 13286.85 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
      "    num_rows: 600\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:483: FutureWarning: DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postprocess_qa Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id'],\n",
      "    num_rows: 600\n",
      "})\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column id not in the dataset. Current columns in the dataset: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Reader 모델 추론 실행\u001b[39;00m\n\u001b[1;32m     19\u001b[0m reader_model \u001b[38;5;241m=\u001b[39m Reader(model_args, data_args, training_args, dataset)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mreader_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/level2-mrc-nlp-07/src/reader/model/reader.py:82\u001b[0m, in \u001b[0;36mReader.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluation results: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_metrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mdo_predict:\n\u001b[0;32m---> 82\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult_saver\u001b[38;5;241m.\u001b[39msave_predictions(predictions)\n",
      "File \u001b[0;32m~/level2-mrc-nlp-07/src/reader/model/trainer_manager.py:131\u001b[0m, in \u001b[0;36mTrainerManager.run_prediction\u001b[0;34m(self, trainer, test_dataset, test_examples)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_prediction\u001b[39m(\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    117\u001b[0m     trainer: QuestionAnsweringTrainer,\n\u001b[1;32m    118\u001b[0m     test_dataset: Dataset,\n\u001b[1;32m    119\u001b[0m     test_examples: Dataset,\n\u001b[1;32m    120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m    모델 예측을 실행합니다.\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        Any: 예측 결과.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_examples\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/level2-mrc-nlp-07/src/reader/model/trainer_qa.py:150\u001b[0m, in \u001b[0;36mQuestionAnsweringTrainer.predict\u001b[0;34m(self, test_dataset, test_examples, ignore_keys)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     test_dataset: datasets\u001b[38;5;241m.\u001b[39mDataset,\n\u001b[1;32m    136\u001b[0m     test_examples: datasets\u001b[38;5;241m.\u001b[39mDataset,\n\u001b[1;32m    137\u001b[0m     ignore_keys: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    138\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    예측 작업을 수행하는 메서드입니다.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m        Union[Dict[str, Any], Any]: 후처리된 예측값 또는 예측 결과.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shared_evaluate_or_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_predict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/level2-mrc-nlp-07/src/reader/model/trainer_qa.py:85\u001b[0m, in \u001b[0;36mQuestionAnsweringTrainer._shared_evaluate_or_predict\u001b[0;34m(self, dataset, examples, description, ignore_keys, is_predict)\u001b[0m\n\u001b[1;32m     79\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mset_format(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     81\u001b[0m         columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_process_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m preds\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/level2-mrc-nlp-07/src/reader/data_controller/data_processor.py:157\u001b[0m, in \u001b[0;36mDataPostProcessor.process\u001b[0;34m(cls, data_args, tokenizer, *datasets)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_args, \u001b[38;5;241m*\u001b[39mdatasets, tokenizer: AutoTokenizer \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# Post-processing: start logits과 end logits을 original context의 정답과 match시킵니다.\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpostprocess_qa_predictions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_answer_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_answer_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Metric을 구할 수 있도록 Format을 맞춰줍니다.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     formatted_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    166\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: k, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_text\u001b[39m\u001b[38;5;124m'\u001b[39m: v} \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    167\u001b[0m     ]\n",
      "File \u001b[0;32m~/level2-mrc-nlp-07/src/reader/data_controller/postprocess_qa.py:86\u001b[0m, in \u001b[0;36mpostprocess_qa_predictions\u001b[0;34m(examples, features, predictions, version_2_with_negative, n_best_size, max_answer_length, null_score_diff_threshold, output_dir, prefix, is_world_process_zero)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# example과 mapping되는 feature 생성\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpostprocess_qa \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(examples))\n\u001b[0;32m---> 86\u001b[0m example_id_to_index \u001b[38;5;241m=\u001b[39m {k: i \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mexamples\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)}\n\u001b[1;32m     87\u001b[0m features_per_example \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2762\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2761\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2746\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2744\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2745\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2746\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2747\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2748\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2749\u001b[0m )\n\u001b[1;32m   2750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:590\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    588\u001b[0m         _raise_bad_key_type(key)\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 590\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:527\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 527\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column id not in the dataset. Current columns in the dataset: ['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'example_id']\""
     ]
    }
   ],
   "source": [
    "# Argument 설정\n",
    "model_args = ModelArguments()\n",
    "data_args = DataTrainingArguments()\n",
    "training_args = TrainingArguments(output_dir='outputs/test_dataset')\n",
    "\n",
    "# 추론 관련 설정\n",
    "training_args.do_train = False\n",
    "training_args.do_eval = False\n",
    "training_args.do_predict = True\n",
    "\n",
    "# 학습된 모델 경로 및 테스트 데이터셋 경로 설정\n",
    "model_args.model_name_or_path = 'outputs/models/train_dataset'\n",
    "data_args.dataset_name = 'data/test_dataset'  # 추론에 사용할 테스트 데이터셋 경로\n",
    "\n",
    "# 테스트 데이터셋 로드\n",
    "dataset = DatasetDict.load_from_disk('data/test_dataset')\n",
    "\n",
    "# Reader 모델 추론 실행\n",
    "reader_model = Reader(model_args, data_args, training_args, dataset)\n",
    "reader_model.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
