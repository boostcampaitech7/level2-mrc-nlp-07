{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Dataset 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "        num_rows: 3952\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "})\n",
      "**************************************** query dataset ****************************************\n",
      "Dataset({\n",
      "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "    num_rows: 4192\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, load_from_disk\n",
    "from src.retriever.retrieval.sparse_retrieval import SparseRetrieval\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "org_dataset = load_from_disk('data/train_dataset')\n",
    "print(org_dataset)\n",
    "full_ds = concatenate_datasets(\n",
    "        [\n",
    "            org_dataset[\"train\"].flatten_indices(),\n",
    "            org_dataset[\"validation\"].flatten_indices(),\n",
    "        ]\n",
    "    )  # train dev 를 합친 4192 개 질문에 대해 모두 테스트\n",
    "print(\"*\" * 40, \"query dataset\", \"*\" * 40)\n",
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever 선언 및 Retriever 진행\n",
    "* 아마 초기엔 학습 진행하는데 시간 좀 걸릴예정\n",
    "    * 약 15분정도 소요(승범 컴 기준)\n",
    "    * 노션에 mrc/검증결과/Sparse 처리에 가장 좋은 bm25올려났으므로 그거 data 폴더 안에 넣으면 실행가능."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n",
      "Building bm25 embedding...\n",
      "Start Initializing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...: 100%|██████████| 56737/56737 [02:20<00:00, 405.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating n-grams and building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-grams: 100%|██████████| 56737/56737 [00:35<00:00, 1590.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mode : bm25\n",
      "End Initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25:   7%|▋         | 4/57 [01:51<23:25, 26.51s/it]"
     ]
    }
   ],
   "source": [
    "# 위에서 선언한거 가져오기 Retriever\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", use_fast=False,)\n",
    "retriever = SparseRetrieval(\n",
    "    tokenize_fn=tokenizer.tokenize,\n",
    "    data_path=\"./data/\",\n",
    "    context_path=\"wikipedia_documents.json\",\n",
    "    mode = \"bm25\",\n",
    "    max_feature=1000000,\n",
    "    ngram_range=(1,2),\n",
    "    #tokenized_docs = tokenized_docs,\n",
    ")\n",
    "retriever.get_sparse_embedding()\n",
    "\n",
    "# Top k 조절.\n",
    "df = retriever.retrieve(full_ds, topk=10)\n",
    "\n",
    "# 여기서 df가 reader가 사용할 데이터 프레임입니다.\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* question : 질문\n",
    "* id : mrc 번호\n",
    "* retrieval_context : retrieval 결과로 나온 topk를 하나의 text로 만든것\n",
    "* original_context : 실제 retrieval 정답\n",
    "* answers: dict 형태\n",
    "    * answers_start : original_context에서 어디에서 시작하는지\n",
    "    * text: 정답의 text값이 뭔지 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13059"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['retrieval_context'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever 결과물 DatasetDict로 저장\n",
    "reader는 데이터셋을 DatasetDict형태로 받으므로, df에서 변환이 필요하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 20\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m'''# df를 Dataset으로\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mretriever_dataset = Dataset.from_pandas(df)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03mdataset_dict['validation'] = dataset_dict['validation'].remove_columns(['original_context']).rename_column('retrieval_context', 'context')\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 3. DataFrame을 각각 train, validation에 맞게 나눔\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_context\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_context\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     21\u001b[0m validation_df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretrieval_context\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswers\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretrieval_context\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 4. DataFrame을 각각 Dataset으로 변환\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# df를 Dataset으로\n",
    "retriever_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# DatasetDict로 변환 (train과 validation을 동일 데이터로 초기화)\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': retriever_dataset,\n",
    "    'validation': retriever_dataset\n",
    "})\n",
    "\n",
    "# train 스플릿: 'original_context'를 'context'로 사용\n",
    "dataset_dict['train'] = dataset_dict['train'].remove_columns(['retrieval_context']).rename_column('original_context', 'context')\n",
    "\n",
    "# validation 스플릿: 'retrieval_context'를 'context'로 사용\n",
    "dataset_dict['validation'] = dataset_dict['validation'].remove_columns(['original_context']).rename_column('retrieval_context', 'context')\n",
    "\n",
    "print(dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18aea5addb0c4ba2a641775122d34056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4192 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 별도 파일로 저장\n",
    "dataset_dict.save_to_disk('outputs/bm25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "--do_train requires a train dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput directory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, training_args\u001b[38;5;241m.\u001b[39moutput_dir)\n\u001b[1;32m     27\u001b[0m reader_model \u001b[38;5;241m=\u001b[39m Reader(model_args, data_args, training_args, dataset)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mreader_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/level2-mrc-nlp-07/src/reader/model/reader.py:54\u001b[0m, in \u001b[0;36mReader.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m validate_flags(training_args\u001b[38;5;241m.\u001b[39mdo_train, training_args\u001b[38;5;241m.\u001b[39mdo_eval, training_args\u001b[38;5;241m.\u001b[39mdo_predict)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# 데이터 전처리\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdata_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mdo_train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     55\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mload_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mdo_eval \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     56\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mload_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mdo_predict \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/level2-mrc-nlp-07/src/reader/data_controller/data_handler.py:77\u001b[0m, in \u001b[0;36mDataHandler.load_data\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid type: must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--do_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     79\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m datasets\n",
      "\u001b[0;31mValueError\u001b[0m: --do_train requires a train dataset"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser\n",
    "from transformers import TrainingArguments\n",
    "from datasets import DatasetDict\n",
    "\n",
    "from src.reader.model.reader import Reader\n",
    "from src.utils.arguments import DataTrainingArguments, ModelArguments\n",
    "\n",
    "#args 로드\n",
    "model_args = ModelArguments()\n",
    "data_args =  DataTrainingArguments()\n",
    "training_args = TrainingArguments(output_dir='outputs')\n",
    "\n",
    "# 세부 세팅\n",
    "# model_args.model_name_or_path = <모델경로/huggingface>\n",
    "\n",
    "training_args.do_train =True\n",
    "training_args.do_eval = True\n",
    "\n",
    "# 디스크로 데이터셋 불러오기\n",
    "# dataset = DatasetDict.load_from_disk('outputs/bm25')\n",
    "\n",
    "#데이터셋 바로 받아올 경우\n",
    "dataset = dataset_dict\n",
    "\n",
    "# output_dir은 training_args에 설정된 값을 사용\n",
    "print(\"Output directory:\", training_args.output_dir)\n",
    "\n",
    "\n",
    "\n",
    "reader_model = Reader(model_args, data_args, training_args, dataset)\n",
    "reader_model.run()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "level2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
