{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\승범 pc\\Desktop\\study\\deeplearning\\naver_boot\\level2-mrc-nlp-07\\level2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "        num_rows: 3952\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "        num_rows: 240\n",
      "    })\n",
      "})\n",
      "**************************************** query dataset ****************************************\n",
      "Dataset({\n",
      "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "    num_rows: 4192\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, load_from_disk\n",
    "from src.retriever.retrieval.retrieval import SparseRetrieval\n",
    "from retriever.embedding.flag_embedding import DenseRetrieval\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from src.retriever.score.ranking import check_original_in_context, calculate_reverse_rank_score, calculate_linear_score\n",
    "org_dataset = load_from_disk('./data/train_dataset')\n",
    "print(org_dataset)\n",
    "full_ds = concatenate_datasets(\n",
    "        [\n",
    "            org_dataset[\"train\"].flatten_indices(),\n",
    "            org_dataset[\"validation\"].flatten_indices(),\n",
    "        ]\n",
    "    )  # train dev 를 합친 4192 개 질문에 대해 모두 테스트\n",
    "print(\"*\" * 40, \"query dataset\", \"*\" * 40)\n",
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"\n",
    "context_path = \"wikipedia_documents.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...:   0%|          | 0/56737 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1337 > 512). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing...: 100%|██████████| 56737/56737 [01:26<00:00, 659.58it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "docs = list(dict.fromkeys([v[\"text\"] for v in wiki.values()]))\n",
    "print(f\"Lengths of unique contexts : {len(docs)}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\", use_fast=False,)\n",
    "tokenized_docs = [tokenizer(doc) for doc in tqdm(docs, desc=\"Tokenizing...\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n",
      "b: 0.25 & k1 : 1.1\n",
      "Building bm25 embedding...\n",
      "Start Initializing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...: 100%|██████████| 56737/56737 [01:14<00:00, 758.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating n-grams and building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-grams: 100%|██████████| 56737/56737 [00:22<00:00, 2473.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mode : bm25\n",
      "End Initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25: 100%|██████████| 57/57 [07:20<00:00,  7.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish BM25 Embedding\n",
      "bm25 embedding shape: (56737, 200000)\n",
      "(4192, 200000) (56737, 200000)\n",
      "result shape : (4192, 56737)\n",
      "[query exhaustive search] done in 19.765 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval: 100%|██████████| 4192/4192 [00:00<00:00, 10695.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct retrieval 0.8322996183206107\n",
      "reverse rank retrieval 0.48521718441181044\n",
      "linear retrieval 0.752088091571962\n",
      "Lengths of unique contexts : 56737\n",
      "b: 0.25 & k1 : 1.3\n",
      "Building bm25 embedding...\n",
      "Start Initializing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...: 100%|██████████| 56737/56737 [01:15<00:00, 754.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating n-grams and building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-grams: 100%|██████████| 56737/56737 [00:22<00:00, 2484.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mode : bm25\n",
      "End Initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25: 100%|██████████| 57/57 [07:18<00:00,  7.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish BM25 Embedding\n",
      "bm25 embedding shape: (56737, 200000)\n",
      "(4192, 200000) (56737, 200000)\n",
      "result shape : (4192, 56737)\n",
      "[query exhaustive search] done in 19.381 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval: 100%|██████████| 4192/4192 [00:00<00:00, 11050.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct retrieval 0.8356393129770993\n",
      "reverse rank retrieval 0.4890297805197442\n",
      "linear retrieval 0.7521429663755775\n",
      "Lengths of unique contexts : 56737\n",
      "b: 0.25 & k1 : 1.5\n",
      "Building bm25 embedding...\n",
      "Start Initializing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...: 100%|██████████| 56737/56737 [01:14<00:00, 757.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating n-grams and building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-grams: 100%|██████████| 56737/56737 [00:22<00:00, 2502.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mode : bm25\n",
      "End Initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25: 100%|██████████| 57/57 [07:17<00:00,  7.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish BM25 Embedding\n",
      "bm25 embedding shape: (56737, 200000)\n",
      "(4192, 200000) (56737, 200000)\n",
      "result shape : (4192, 56737)\n",
      "[query exhaustive search] done in 20.525 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval: 100%|██████████| 4192/4192 [00:00<00:00, 10392.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct retrieval 0.8361164122137404\n",
      "reverse rank retrieval 0.484263658173665\n",
      "linear retrieval 0.7505740053221104\n",
      "Lengths of unique contexts : 56737\n",
      "b: 0.25 & k1 : 1.7\n",
      "Building bm25 embedding...\n",
      "Start Initializing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...: 100%|██████████| 56737/56737 [01:16<00:00, 737.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating n-grams and building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-grams: 100%|██████████| 56737/56737 [00:23<00:00, 2399.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mode : bm25\n",
      "End Initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25: 100%|██████████| 57/57 [07:40<00:00,  8.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish BM25 Embedding\n",
      "bm25 embedding shape: (56737, 200000)\n",
      "(4192, 200000) (56737, 200000)\n",
      "result shape : (4192, 56737)\n",
      "[query exhaustive search] done in 21.169 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sparse retrieval: 100%|██████████| 4192/4192 [00:00<00:00, 11078.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct retrieval 0.8327767175572519\n",
      "reverse rank retrieval 0.4825900502852024\n",
      "linear retrieval 0.7487632477260269\n",
      "Lengths of unique contexts : 56737\n",
      "b: 0.25 & k1 : 1.9\n",
      "Building bm25 embedding...\n",
      "Start Initializing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing...: 100%|██████████| 56737/56737 [01:19<00:00, 712.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating n-grams and building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-grams: 100%|██████████| 56737/56737 [00:28<00:00, 2025.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mode : bm25\n",
      "End Initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25:  54%|█████▍    | 31/57 [04:30<03:53,  9.00s/it]"
     ]
    }
   ],
   "source": [
    "# 위에서 선언한거 가져오기 Retriever\n",
    "for b in [0.25, 0.5, 0.75, 1.0]:\n",
    "    for k1 in [1.1, 1.3, 1.5, 1.7, 1.9]:\n",
    "        retriever = SparseRetrieval(\n",
    "            tokenize_fn=tokenizer.tokenize,\n",
    "            data_path=\"./data/\",\n",
    "            context_path=\"wikipedia_documents.json\",\n",
    "            mode = \"bm25\",\n",
    "            max_feature=200000,\n",
    "            ngram_range=(1,2),\n",
    "            tokenized_docs = tokenized_docs,\n",
    "            b=b,\n",
    "            k1=k1,\n",
    "        )\n",
    "        print(f'b: {b} & k1 : {k1}')\n",
    "        retriever.get_sparse_embedding()\n",
    "        df = retriever.retrieve(full_ds, topk=10)\n",
    "        retriever.get_score(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of unique contexts : 56737\n",
      "Building bm25 embedding...\n",
      "Start Initializing...\n",
      "Pass Tokenizing\n",
      "Generating n-grams and building vocabulary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating n-grams: 100%|██████████| 56737/56737 [00:00<00:00, 169690.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current mode : bm25\n",
      "End Initialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating BM25: 100%|██████████| 57/57 [00:00<00:00, 139.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish BM25 Embedding\n",
      "New embeddings calculated and saved.\n",
      "bm25 embedding shape: (56737, 1)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "query_vecs가 제대로 변환되지않음.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m retriever \u001b[38;5;241m=\u001b[39m SparseRetrieval(\n\u001b[0;32m      2\u001b[0m             tokenize_fn\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mtokenize,\n\u001b[0;32m      3\u001b[0m             data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m             k1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m,\n\u001b[0;32m     11\u001b[0m         )\n\u001b[0;32m     12\u001b[0m retriever\u001b[38;5;241m.\u001b[39mget_sparse_embedding()\n\u001b[1;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m retriever\u001b[38;5;241m.\u001b[39mget_score(df)\n",
      "File \u001b[1;32mc:\\Users\\승범 pc\\Desktop\\study\\deeplearning\\naver_boot\\level2-mrc-nlp-07\\src\\retriever\\retrieval\\retrieval.py:165\u001b[0m, in \u001b[0;36mSparseRetrieval.retrieve\u001b[1;34m(self, query_or_dataset, topk)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# 위에서 선언한 contextmanager 들고와서 걸리는 시간 Check!\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m timer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery exhaustive search\u001b[39m\u001b[38;5;124m\"\u001b[39m): \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     doc_scores, doc_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_relevant_doc_bulk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_or_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopk\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# 쿼리와 ID와 내용 저장.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[0;32m    170\u001b[0m     tqdm(query_or_dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparse retrieval: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\승범 pc\\Desktop\\study\\deeplearning\\naver_boot\\level2-mrc-nlp-07\\src\\retriever\\retrieval\\retrieval.py:239\u001b[0m, in \u001b[0;36mSparseRetrieval.get_relevant_doc_bulk\u001b[1;34m(self, queries, k)\u001b[0m\n\u001b[0;32m    237\u001b[0m stage1 \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_embed\u001b[38;5;241m.\u001b[39mtransform(query) \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries]\n\u001b[0;32m    238\u001b[0m query_vecs \u001b[38;5;241m=\u001b[39m vstack(stage1) \u001b[38;5;66;03m# 질문수, 임베딩 차원\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    240\u001b[0m     np\u001b[38;5;241m.\u001b[39msum(query_vecs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    241\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_vecs가 제대로 변환되지않음.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28mprint\u001b[39m(query_vecs\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_embedding\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# 유사도 계산\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: query_vecs가 제대로 변환되지않음."
     ]
    }
   ],
   "source": [
    "retriever = SparseRetrieval(\n",
    "            tokenize_fn=tokenizer.tokenize,\n",
    "            data_path=\"./data/\",\n",
    "            context_path=\"wikipedia_documents.json\",\n",
    "            mode = \"bm25\",\n",
    "            max_feature=200000,\n",
    "            ngram_range=(1,2),\n",
    "            tokenized_docs = tokenized_docs,\n",
    "            b=0.25,\n",
    "            k1=1.1,\n",
    "        )\n",
    "retriever.get_sparse_embedding()\n",
    "df = retriever.retrieve(full_ds, topk=10)\n",
    "retriever.get_score(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = retriever.retrieve(full_ds, topk=10)\n",
    "retriever.get_score(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 데이터 준비\n",
    "data = {\n",
    "    'max_feature': [48600, 50000, 80000, 100000, 200000, 400000, 1000000, 1784976, 2000000],\n",
    "    'Correct': [0.6836832061, 0.7645515267, 0.7924618321, 0.8058206107, 0.8401717557, 0.8590171756, 0.8707061069, 0.8730916031, 0.8289599237],\n",
    "    'Reverse_Rank': [0.3587869386, 0.4144341905, 0.4454477764, 0.4557101427, 0.4905500473, 0.5160778654, 0.5361060586, 0.5403996207, 0.4767265568],\n",
    "    'Linear': [0.5986908422, 0.6757890968, 0.7080769905, 0.7213994905, 0.7603113276, 0.7819702961, 0.7966366694, 0.8001752019, 0.7452705811]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 'no gram' 표시 추가\n",
    "df['label'] = df['max_feature'].astype(str)\n",
    "df.loc[0, 'label'] = '48600 (no gram)'\n",
    "\n",
    "# 데이터 정렬\n",
    "df = df.sort_values('max_feature')\n",
    "\n",
    "# 그래프 스타일 설정\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# 선 그래프 그리기\n",
    "plt.plot(df['max_feature'], df['Correct'], marker='o', label='Correct')\n",
    "plt.plot(df['max_feature'], df['Reverse_Rank'], marker='s', label='Reverse Rank')\n",
    "plt.plot(df['max_feature'], df['Linear'], marker='^', label='Linear')\n",
    "\n",
    "# 그래프 꾸미기\n",
    "plt.xscale('log')  # x축을 로그 스케일로 변경\n",
    "plt.xlabel('Max Feature')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics vs Max Feature')\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "# x축 눈금 설정\n",
    "plt.xticks(df['max_feature'], df['label'], rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "level2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
